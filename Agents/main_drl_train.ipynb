{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from DrlLibs.training import create_parallel_environment, train_drl_agent\n",
    "from DrlLibs.evaluate import evaluate_drl_agent\n",
    "from DrlLibs import create_environment, check_env, DRLResourceSchedulingEnv\n",
    "from Configs import getEnvConfig, visualizeEnvConfig, getPredictorConfig, visualizePredictorConfig\n",
    "from DrlLibs.visualize import plot_training_results\n",
    "from Environment.EnvironmentSim import createEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(simParams, simEnv, save_path, agent_name, algorithm_name: str = \"SAC\", \n",
    "          obvMode=\"perfect\", total_timesteps: int = 20000, \n",
    "          timesteps_per_episode: int = 5000, n_envs: int = 4,\n",
    "          early_stop_threshold: float = 0.05, min_steps_before_stop: int = 1000,\n",
    "          moving_avg_window: int = 100):\n",
    "    \"\"\"Train a DRL agent with early stopping based on moving average.\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        callback: Training callback with metrics\n",
    "        training_time: Time taken for training\n",
    "        env: Training environment (caller should close this)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"{algorithm_name} as Agent config{agent_name}'s Training\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create environment (single or parallel)\n",
    "    print(\"Creating environment...\")\n",
    "    env = create_parallel_environment(simParams, simEnv, obvMode, \n",
    "                                    timesteps_per_episode, n_envs)\n",
    "    \n",
    "    # Check environment (only for single env)\n",
    "    if n_envs == 1:\n",
    "        print(\"Checking environment...\")\n",
    "        check_env(env.unwrapped)\n",
    "        print(\"Environment check passed!\")\n",
    "    else:\n",
    "        print(f\"Created {n_envs} parallel environments\")\n",
    "    \n",
    "    model, callback, training_time = train_drl_agent(\n",
    "        algorithm_name, env, total_timesteps, save_path, agent_name,\n",
    "        early_stop_threshold=early_stop_threshold, \n",
    "        min_steps_before_stop=min_steps_before_stop,\n",
    "        moving_avg_window=moving_avg_window\n",
    "    )\n",
    "    \n",
    "    # Print training summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Algorithm: {algorithm_name}\")\n",
    "    print(f\"Training completed in: {training_time:.2f} seconds\")\n",
    "    print(f\"Total training timesteps: {total_timesteps}\")\n",
    "    \n",
    "    if hasattr(callback, 'cumulative_rewards') and callback.cumulative_rewards:\n",
    "        print(f\"Episodes completed during training: {callback.episodes_seen}\")\n",
    "        print(f\"Final cumulative reward: {callback.total_reward:.4f}\")\n",
    "        print(f\"Average reward per episode: {callback.total_reward / callback.episodes_seen:.4f}\")\n",
    "    else:\n",
    "        print(\"No cumulative reward data captured during training\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "    return model, callback, training_time, env\n",
    "\n",
    "\n",
    "def eval(model, simParams, simEnv, algorithm_name: str = \"SAC\", \n",
    "         deterministic=True,    \n",
    "         obvMode=\"perfect\", timesteps_per_episode: int = 5000, n_steps: int = 1000):\n",
    "    \"\"\"Evaluate a trained DRL agent.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model to evaluate\n",
    "        simParams: Environment simulation parameters\n",
    "        simEnv: Simulation environment\n",
    "        algorithm_name: Name of the algorithm\n",
    "        obvMode: Observation mode\n",
    "        timesteps_per_episode: Number of timesteps per episode\n",
    "        n_steps: Number of steps to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        eval_results: Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"{algorithm_name} Agent Evaluation\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create a clean single environment for evaluation\n",
    "    print(\"Creating evaluation environment...\")\n",
    "    env = DRLResourceSchedulingEnv(\n",
    "        simParams,\n",
    "        simEnv,\n",
    "        obvMode,\n",
    "        n_steps\n",
    "    )\n",
    "    \n",
    "    eval_results = evaluate_drl_agent(\n",
    "        model=model,\n",
    "        env=env,\n",
    "        algorithm_name=algorithm_name,\n",
    "        deterministic=deterministic,\n",
    "        n_steps=n_steps\n",
    "    )\n",
    "    \n",
    "    # Close evaluation environment\n",
    "    env.close()\n",
    "    \n",
    "    # Print evaluation summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Algorithm: {algorithm_name}\")\n",
    "    print(f\"Average evaluation reward: {eval_results['avg_reward']:.4f} ± {eval_results['std_reward']:.4f}\")\n",
    "    print(f\"Average packet loss rate: {eval_results['avg_packet_loss']:.4f} ± {eval_results['std_packet_loss']:.4f}\")\n",
    "    \n",
    "    return eval_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Environment Configuration\n",
      "==================================================\n",
      "Number of Users:        20\n",
      "Window Length:          200\n",
      "Dataflow:               thumb_bk\n",
      "Sigmoid K List:         [0.1, 0.2, 0.3, 0.4, 0.5]\n",
      "Sigmoid S List:         [10.0, 10.0, 10.0, 10.0, 10.0]\n",
      "Resource Bar:           5\n",
      "Bandwidth:              200\n",
      "Sub Agents:             [[1, 1, 1, 1, 1]]\n",
      "User Map:               [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19]]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "configIdx = 3\n",
    "envParams = getEnvConfig(configIdx)\n",
    "visualizeEnvConfig(envParams)\n",
    "trafficDataParentPath = f'../Datasets/TrafficDataset/TrafficData'\n",
    "simEnv = createEnv(envParams, trafficDataParentPath)\n",
    "simEnv.selectMode(mode=\"train\", type=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAC as Agent configconfig3's Training\n",
      "================================================================================\n",
      "Creating environment...\n",
      "Created 4 parallel environments\n",
      "\n",
      "============================================================\n",
      "Training SAC as config3 Agent\n",
      "============================================================\n",
      "Total timesteps: 5000\n",
      "Early stop threshold: 0.05\n",
      "Min steps before stop: 100\n",
      "Moving average window: 100 data points\n",
      "Environment: Parallel environments (details not accessible)\n",
      "Save path: Agents/DRL/SAC.zip\n",
      "Starting training...\n",
      "\n",
      "================================================================================\n",
      "Timestep: 400 | 4 Episode(s) Completed\n",
      "================================================================================\n",
      "  Env  0: Packet Loss Rate = 0.8136\n",
      "  Env  1: Packet Loss Rate = 0.7863\n",
      "  Env  2: Packet Loss Rate = 0.8027\n",
      "  Env  3: Packet Loss Rate = 0.8009\n",
      "--------------------------------------------------------------------------------\n",
      "  Best Loss (this step):    0.7863\n",
      "  Avg Loss (this step):     0.8009\n",
      "  Avg Loss (last 10 eps):   0.8009\n",
      "  Total Episodes So Far:    4\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Timestep: 800 | 4 Episode(s) Completed\n",
      "================================================================================\n",
      "  Env  0: Packet Loss Rate = 0.8017\n",
      "  Env  1: Packet Loss Rate = 0.8155\n",
      "  Env  2: Packet Loss Rate = 0.7809\n",
      "  Env  3: Packet Loss Rate = 0.8006\n",
      "--------------------------------------------------------------------------------\n",
      "  Best Loss (this step):    0.7809\n",
      "  Avg Loss (this step):     0.7997\n",
      "  Avg Loss (last 10 eps):   0.8003\n",
      "  Total Episodes So Far:    8\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Timestep: 1200 | 4 Episode(s) Completed\n",
      "================================================================================\n",
      "  Env  0: Packet Loss Rate = 0.7951\n",
      "  Env  1: Packet Loss Rate = 0.7913\n",
      "  Env  2: Packet Loss Rate = 0.7938\n",
      "  Env  3: Packet Loss Rate = 0.7982\n",
      "--------------------------------------------------------------------------------\n",
      "  Best Loss (this step):    0.7913\n",
      "  Avg Loss (this step):     0.7946\n",
      "  Avg Loss (last 10 eps):   0.7981\n",
      "  Total Episodes So Far:    12\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Timestep: 1600 | 4 Episode(s) Completed\n",
      "================================================================================\n",
      "  Env  0: Packet Loss Rate = 0.8045\n",
      "  Env  1: Packet Loss Rate = 0.8004\n",
      "  Env  2: Packet Loss Rate = 0.7939\n",
      "  Env  3: Packet Loss Rate = 0.7976\n",
      "--------------------------------------------------------------------------------\n",
      "  Best Loss (this step):    0.7939\n",
      "  Avg Loss (this step):     0.7991\n",
      "  Avg Loss (last 10 eps):   0.7956\n",
      "  Total Episodes So Far:    16\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Timestep: 2000 | 4 Episode(s) Completed\n",
      "================================================================================\n",
      "  Env  0: Packet Loss Rate = 0.7969\n",
      "  Env  1: Packet Loss Rate = 0.8010\n",
      "  Env  2: Packet Loss Rate = 0.7877\n",
      "  Env  3: Packet Loss Rate = 0.7936\n",
      "--------------------------------------------------------------------------------\n",
      "  Best Loss (this step):    0.7877\n",
      "  Avg Loss (this step):     0.7948\n",
      "  Avg Loss (last 10 eps):   0.7968\n",
      "  Total Episodes So Far:    20\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Timestep: 2400 | 4 Episode(s) Completed\n",
      "================================================================================\n",
      "  Env  0: Packet Loss Rate = 0.8027\n",
      "  Env  1: Packet Loss Rate = 0.8040\n",
      "  Env  2: Packet Loss Rate = 0.7938\n",
      "  Env  3: Packet Loss Rate = 0.7999\n",
      "--------------------------------------------------------------------------------\n",
      "  Best Loss (this step):    0.7938\n",
      "  Avg Loss (this step):     0.8001\n",
      "  Avg Loss (last 10 eps):   0.7971\n",
      "  Total Episodes So Far:    24\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Timestep: 2800 | 4 Episode(s) Completed\n",
      "================================================================================\n",
      "  Env  0: Packet Loss Rate = 0.8079\n",
      "  Env  1: Packet Loss Rate = 0.8014\n",
      "  Env  2: Packet Loss Rate = 0.7823\n",
      "  Env  3: Packet Loss Rate = 0.8087\n",
      "--------------------------------------------------------------------------------\n",
      "  Best Loss (this step):    0.7823\n",
      "  Avg Loss (this step):     0.8001\n",
      "  Avg Loss (last 10 eps):   0.7982\n",
      "  Total Episodes So Far:    28\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Timestep: 3200 | 4 Episode(s) Completed\n",
      "================================================================================\n",
      "  Env  0: Packet Loss Rate = 0.8024\n",
      "  Env  1: Packet Loss Rate = 0.7979\n",
      "  Env  2: Packet Loss Rate = 0.8081\n",
      "  Env  3: Packet Loss Rate = 0.7963\n",
      "--------------------------------------------------------------------------------\n",
      "  Best Loss (this step):    0.7963\n",
      "  Avg Loss (this step):     0.8012\n",
      "  Avg Loss (last 10 eps):   0.7999\n",
      "  Total Episodes So Far:    32\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Timestep: 3600 | 4 Episode(s) Completed\n",
      "================================================================================\n",
      "  Env  0: Packet Loss Rate = 0.8013\n",
      "  Env  1: Packet Loss Rate = 0.8038\n",
      "  Env  2: Packet Loss Rate = 0.8089\n",
      "  Env  3: Packet Loss Rate = 0.8201\n",
      "--------------------------------------------------------------------------------\n",
      "  Best Loss (this step):    0.8013\n",
      "  Avg Loss (this step):     0.8085\n",
      "  Avg Loss (last 10 eps):   0.8030\n",
      "  Total Episodes So Far:    36\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Timestep: 4000 | 4 Episode(s) Completed\n",
      "================================================================================\n",
      "  Env  0: Packet Loss Rate = 0.8078\n",
      "  Env  1: Packet Loss Rate = 0.8024\n",
      "  Env  2: Packet Loss Rate = 0.8131\n",
      "  Env  3: Packet Loss Rate = 0.8173\n",
      "--------------------------------------------------------------------------------\n",
      "  Best Loss (this step):    0.8024\n",
      "  Avg Loss (this step):     0.8102\n",
      "  Avg Loss (last 10 eps):   0.8079\n",
      "  Total Episodes So Far:    40\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Timestep: 4400 | 4 Episode(s) Completed\n",
      "================================================================================\n",
      "  Env  0: Packet Loss Rate = 0.8044\n",
      "  Env  1: Packet Loss Rate = 0.8078\n",
      "  Env  2: Packet Loss Rate = 0.8005\n",
      "  Env  3: Packet Loss Rate = 0.8068\n",
      "--------------------------------------------------------------------------------\n",
      "  Best Loss (this step):    0.8005\n",
      "  Avg Loss (this step):     0.8049\n",
      "  Avg Loss (last 10 eps):   0.8089\n",
      "  Total Episodes So Far:    44\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Timestep: 4800 | 4 Episode(s) Completed\n",
      "================================================================================\n",
      "  Env  0: Packet Loss Rate = 0.8275\n",
      "  Env  1: Packet Loss Rate = 0.8149\n",
      "  Env  2: Packet Loss Rate = 0.8138\n",
      "  Env  3: Packet Loss Rate = 0.7962\n",
      "--------------------------------------------------------------------------------\n",
      "  Best Loss (this step):    0.7962\n",
      "  Avg Loss (this step):     0.8131\n",
      "  Avg Loss (last 10 eps):   0.8102\n",
      "  Total Episodes So Far:    48\n",
      "================================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Training Summary\n",
      "============================================================\n",
      "Training completed normally\n",
      "Total timesteps: 5000\n",
      "Total episodes: 48\n",
      "\n",
      "============================================================\n",
      "Packet Loss Rate Statistics (Stochastic/With Exploration)\n",
      "============================================================\n",
      "Mean packet loss rate: 0.8023\n",
      "Std packet loss rate: 0.0092\n",
      "Min packet loss rate: 0.7809\n",
      "Max packet loss rate: 0.8275\n",
      "Last 10 episodes avg loss: 0.8102\n",
      "\n",
      "Note: Training uses stochastic actions (with exploration noise).\n",
      "      For fair comparison, evaluate with deterministic=False\n",
      "\n",
      "Training time: 27.62 seconds\n",
      "Model saved to: Agents/DRL/SAC/config3.zip\n",
      "\n",
      "================================================================================\n",
      "TRAINING SUMMARY\n",
      "================================================================================\n",
      "Algorithm: SAC\n",
      "Training completed in: 27.62 seconds\n",
      "Total training timesteps: 5000\n",
      "Episodes completed during training: 48\n",
      "Final cumulative reward: 242.7788\n",
      "Average reward per episode: 5.0579\n",
      "================================================================================\n",
      "SAC Agent Evaluation\n",
      "================================================================================\n",
      "Creating evaluation environment...\n",
      "\n",
      "============================================================\n",
      "Evaluating SAC Agent for 1000 steps\n",
      "Mode: Deterministic\n",
      "============================================================\n",
      "Evaluation Results:\n",
      "  Steps Completed: 1000\n",
      "  Average Reward: 0.2009 ± 0.0913\n",
      "  Average Packet Loss: 0.8059 ± 0.0089\n",
      "\n",
      "================================================================================\n",
      "EVALUATION SUMMARY\n",
      "================================================================================\n",
      "Algorithm: SAC\n",
      "Average evaluation reward: 0.2009 ± 0.0913\n",
      "Average packet loss rate: 0.8059 ± 0.0089\n"
     ]
    }
   ],
   "source": [
    "# Configuration - change these parameters as needed\n",
    "ALGORITHM = \"SAC\"           # Options: \"SAC\", \"PPO\", \"A2C\", \"TD3\", \"DQN\"\n",
    "TIMESTEPS = 5000       # Training timesteps\n",
    "TIMESTEPS_PER_EPISODE = 100\n",
    "N_ENVS = 4\n",
    "SAVEPATH = f\"Agents/DRL/{ALGORITHM}\"\n",
    "AGENTNAME = f\"config{configIdx}\"\n",
    "OBVMODE = \"predicted\"\n",
    "\n",
    "# Step 1: Train the model\n",
    "model, callback, training_time, env = train(\n",
    "    simParams=envParams,\n",
    "    simEnv=simEnv,\n",
    "    save_path=SAVEPATH,\n",
    "    agent_name=AGENTNAME,\n",
    "    algorithm_name=ALGORITHM,\n",
    "    obvMode=OBVMODE,\n",
    "    total_timesteps=TIMESTEPS,\n",
    "    timesteps_per_episode=TIMESTEPS_PER_EPISODE,\n",
    "    n_envs=N_ENVS,\n",
    "    early_stop_threshold=0.05,\n",
    "    min_steps_before_stop=100,\n",
    "    moving_avg_window=100\n",
    ")\n",
    "\n",
    "# Close training environment\n",
    "env.close()\n",
    "\n",
    "# Step 2: Evaluate the model\n",
    "results = eval(\n",
    "    model=model,\n",
    "    simParams=envParams,\n",
    "    simEnv=simEnv,\n",
    "    algorithm_name=ALGORITHM,\n",
    "    obvMode=OBVMODE,\n",
    "    deterministic=True,\n",
    "    timesteps_per_episode=TIMESTEPS_PER_EPISODE,\n",
    "    n_steps=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating visualizations...\n",
      "Plotting training progress: 312 data points over 4992 timesteps\n",
      "Plot saved to: sac_training_progress.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 3: Visualize results\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "try:\n",
    "    plot_training_results(callback, results, ALGORITHM, save_plots=True)\n",
    "except Exception as e:\n",
    "    print(f\"Visualization failed: {e}\")\n",
    "    print(\"Training data summary:\")\n",
    "    if hasattr(callback, 'cumulative_rewards') and callback.cumulative_rewards:\n",
    "        print(f\"  Cumulative reward data points: {len(callback.cumulative_rewards)}\")\n",
    "        print(f\"  Final cumulative reward: {callback.cumulative_rewards[-1]:.4f}\")\n",
    "    else:\n",
    "        print(\"  No cumulative reward data captured\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic_predictor_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../../')\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from src.difsched.agents.drl import create_parallel_environment, train_drl_agent\n",
    "from src.difsched.agents.drl import evaluate_drl_agent\n",
    "from src.difsched.agents.drl import create_environment, DRLResourceSchedulingEnv\n",
    "from src.difsched.config import getEnvConfig, visualizeEnvConfig\n",
    "from src.difsched.agents.drl.visualize import plot_training_results\n",
    "from src.difsched.env.EnvironmentSim import createEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(simParams, simEnv, save_path, agent_name, algorithm_name: str = \"SAC\", \n",
    "          obvMode=\"perfect\", total_timesteps: int = 20000, \n",
    "          timesteps_per_episode: int = 5000, n_envs: int = 4,\n",
    "          early_stop_threshold: float = 0.05, min_steps_before_stop: int = 1000,\n",
    "          moving_avg_window: int = 100):\n",
    "    \"\"\"Train a DRL agent with early stopping based on moving average.\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        callback: Training callback with metrics\n",
    "        training_time: Time taken for training\n",
    "        env: Training environment (caller should close this)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"{algorithm_name} as Agent config{agent_name}'s Training\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create environment (single or parallel)\n",
    "    print(\"Creating environment...\")\n",
    "    env = create_parallel_environment(simParams, simEnv, obvMode, \n",
    "                                    timesteps_per_episode, n_envs)\n",
    "    \n",
    "    # Check environment (only for single env)\n",
    "    if n_envs == 1:\n",
    "        print(\"Checking environment...\")\n",
    "        check_env(env.unwrapped)\n",
    "        print(\"Environment check passed!\")\n",
    "    else:\n",
    "        print(f\"Created {n_envs} parallel environments\")\n",
    "    \n",
    "    model, callback, training_time = train_drl_agent(\n",
    "        algorithm_name, env, total_timesteps, save_path, agent_name,\n",
    "        early_stop_threshold=early_stop_threshold, \n",
    "        min_steps_before_stop=min_steps_before_stop,\n",
    "        moving_avg_window=moving_avg_window\n",
    "    )\n",
    "    \n",
    "    # Print training summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Algorithm: {algorithm_name}\")\n",
    "    print(f\"Training completed in: {training_time:.2f} seconds\")\n",
    "    print(f\"Total training timesteps: {total_timesteps}\")\n",
    "    \n",
    "    if hasattr(callback, 'cumulative_rewards') and callback.cumulative_rewards:\n",
    "        print(f\"Episodes completed during training: {callback.episodes_seen}\")\n",
    "        print(f\"Final cumulative reward: {callback.total_reward:.4f}\")\n",
    "        print(f\"Average reward per episode: {callback.total_reward / callback.episodes_seen:.4f}\")\n",
    "    else:\n",
    "        print(\"No cumulative reward data captured during training\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "    return model, callback, training_time, env\n",
    "\n",
    "\n",
    "def eval(model, simParams, simEnv, algorithm_name: str = \"SAC\", \n",
    "         deterministic=True,    \n",
    "         obvMode=\"perfect\", timesteps_per_episode: int = 5000, n_steps: int = 1000):\n",
    "    \"\"\"Evaluate a trained DRL agent.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model to evaluate\n",
    "        simParams: Environment simulation parameters\n",
    "        simEnv: Simulation environment\n",
    "        algorithm_name: Name of the algorithm\n",
    "        obvMode: Observation mode\n",
    "        timesteps_per_episode: Number of timesteps per episode\n",
    "        n_steps: Number of steps to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        eval_results: Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"{algorithm_name} Agent Evaluation\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create a clean single environment for evaluation\n",
    "    print(\"Creating evaluation environment...\")\n",
    "    env = DRLResourceSchedulingEnv(\n",
    "        simParams,\n",
    "        simEnv,\n",
    "        obvMode,\n",
    "        n_steps\n",
    "    )\n",
    "    \n",
    "    eval_results = evaluate_drl_agent(\n",
    "        model=model,\n",
    "        env=env,\n",
    "        algorithm_name=algorithm_name,\n",
    "        deterministic=deterministic,\n",
    "        n_steps=n_steps\n",
    "    )\n",
    "    \n",
    "    # Close evaluation environment\n",
    "    env.close()\n",
    "    \n",
    "    # Print evaluation summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Algorithm: {algorithm_name}\")\n",
    "    print(f\"Average evaluation reward: {eval_results['avg_reward']:.4f} ± {eval_results['std_reward']:.4f}\")\n",
    "    print(f\"Average packet loss rate: {eval_results['avg_packet_loss']:.4f} ± {eval_results['std_packet_loss']:.4f}\")\n",
    "    \n",
    "    return eval_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Environment Configuration\n",
      "==================================================\n",
      "Number of Users:        20\n",
      "Window Length:          200\n",
      "Dataflow:               thumb_bk\n",
      "Sigmoid K List:         [0.1, 0.2, 0.3, 0.4, 0.5]\n",
      "Sigmoid S List:         [10.0, 10.0, 10.0, 10.0, 10.0]\n",
      "Resource Bar:           5\n",
      "Bandwidth:              200\n",
      "Sub Agents:             [[1, 1, 1, 1, 1]]\n",
      "User Map:               [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19]]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "configIdx = 3\n",
    "envParams = getEnvConfig(configIdx)\n",
    "visualizeEnvConfig(envParams)\n",
    "trafficDataParentPath = f'../../../../data/processed/traffic'\n",
    "simEnv = createEnv(envParams, trafficDataParentPath)\n",
    "simEnv.selectMode(mode=\"train\", type=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - change these parameters as needed\n",
    "ALGORITHM = \"SAC\"           # Options: \"SAC\", \"PPO\", \"A2C\", \"TD3\", \"DQN\"\n",
    "TIMESTEPS = 5000       # Training timesteps\n",
    "TIMESTEPS_PER_EPISODE = 100\n",
    "N_ENVS = 8\n",
    "SAVEPATH = f\"../../../../data/results/drl/{ALGORITHM}\"\n",
    "AGENTNAME = f\"config{configIdx}\"\n",
    "OBVMODE = \"predicted\"\n",
    "\n",
    "# Step 1: Train the model\n",
    "model, callback, training_time, env = train(\n",
    "    simParams=envParams,\n",
    "    simEnv=simEnv,\n",
    "    save_path=SAVEPATH,\n",
    "    agent_name=AGENTNAME,\n",
    "    algorithm_name=ALGORITHM,\n",
    "    obvMode=OBVMODE,\n",
    "    total_timesteps=TIMESTEPS,\n",
    "    timesteps_per_episode=TIMESTEPS_PER_EPISODE,\n",
    "    n_envs=N_ENVS,\n",
    "    early_stop_threshold=0.05,\n",
    "    min_steps_before_stop=100,\n",
    "    moving_avg_window=100\n",
    ")\n",
    "\n",
    "# Close training environment\n",
    "env.close()\n",
    "\n",
    "# Step 2: Evaluate the model\n",
    "results = eval(\n",
    "    model=model,\n",
    "    simParams=envParams,\n",
    "    simEnv=simEnv,\n",
    "    algorithm_name=ALGORITHM,\n",
    "    obvMode=OBVMODE,\n",
    "    deterministic=True,\n",
    "    timesteps_per_episode=TIMESTEPS_PER_EPISODE,\n",
    "    n_steps=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating visualizations...\n",
      "Plotting training progress: 350 data points over 5600 timesteps\n",
      "Plot saved to: sac_training_progress.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 3: Visualize results\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "try:\n",
    "    plot_training_results(callback, results, ALGORITHM, save_plots=True)\n",
    "except Exception as e:\n",
    "    print(f\"Visualization failed: {e}\")\n",
    "    print(\"Training data summary:\")\n",
    "    if hasattr(callback, 'cumulative_rewards') and callback.cumulative_rewards:\n",
    "        print(f\"  Cumulative reward data points: {len(callback.cumulative_rewards)}\")\n",
    "        print(f\"  Final cumulative reward: {callback.cumulative_rewards[-1]:.4f}\")\n",
    "    else:\n",
    "        print(\"  No cumulative reward data captured\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic_predictor_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

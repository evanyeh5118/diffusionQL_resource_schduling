{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f9d306",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'EnvLibs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDrlLibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_parallel_environment, train_drl_agent\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDrlLibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_drl_agent\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDrlLibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_environment, check_env\n",
      "File \u001b[1;32mc:\\Users\\Ye\\Documents\\YuYeh_Documents\\L2S\\Projects\\diffusion_resource_schduling_intra_slice\\DrlLibs\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDRL_EnvSim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DRLResourceSchedulingEnv\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDRL_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     get_algorithm_config, \n\u001b[0;32m      4\u001b[0m     get_training_config,\n\u001b[0;32m      5\u001b[0m     print_algorithm_info\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_drl_agent, create_environment, check_env\n",
      "File \u001b[1;32mc:\\Users\\Ye\\Documents\\YuYeh_Documents\\L2S\\Projects\\diffusion_resource_schduling_intra_slice\\DrlLibs\\DRL_EnvSim.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spaces\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Any, Tuple, Optional\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mEnvLibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Environment, RewardKernel, TrafficGenerator\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDRLResourceSchedulingEnv\u001b[39;00m(gym\u001b[38;5;241m.\u001b[39mEnv):\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    Gym environment for the MDP-based resource scheduling problem.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    This environment wraps the MdpSchedule components and provides a standard\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    Gym interface for RL agents to learn optimal resource allocation policies.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'EnvLibs'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from DrlLibs.training import create_parallel_environment, train_drl_agent\n",
    "from DrlLibs.evaluate import evaluate_drl_agent\n",
    "from DrlLibs import create_environment, check_env\n",
    "from Configs import getEnvConfig, visualizeEnvConfig, getPredictorConfig, visualizePredictorConfig\n",
    "from Environment.EnvironmentSim import createEnv\n",
    "from DrlLibs.visualize import plot_training_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main_single_experiment(simParams, simEnv, save_path, agent_name, algorithm_name: str = \"SAC\", \n",
    "                          obvMode=\"perfect\", total_timesteps: int = 20000, \n",
    "                          timesteps_per_episode: int = 5000, n_envs: int = 4,\n",
    "                          moving_avg_window: int = 100):\n",
    "    \"\"\"Single experiment function - same as your original main function.\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"{algorithm_name} as Agent config{agent_name}'s Training and Evaluation\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create environment (single or parallel)\n",
    "    print(\"Creating environment...\")\n",
    "    env = create_parallel_environment(simParams, simEnv, obvMode, \n",
    "                                    timesteps_per_episode, n_envs)\n",
    "    \n",
    "    # Check environment (only for single env)\n",
    "    if n_envs == 1:\n",
    "        print(\"Checking environment...\")\n",
    "        check_env(env.unwrapped)\n",
    "        print(\"Environment check passed!\")\n",
    "    else:\n",
    "        print(f\"Created {n_envs} parallel environments\")\n",
    "    \n",
    "    # Remove early stopping parameters from here as well\n",
    "    model, callback, training_time = train_drl_agent(\n",
    "        algorithm_name, env, total_timesteps, save_path, agent_name,\n",
    "        moving_avg_window=moving_avg_window\n",
    "    )\n",
    "    \n",
    "    # Create a clean single environment for evaluation\n",
    "    print(\"Creating evaluation environment...\")\n",
    "    eval_env = create_environment(simParams, simEnv, obvMode, timesteps_per_episode)\n",
    "    \n",
    "    # Evaluate DRL agent\n",
    "    eval_results = evaluate_drl_agent(model, eval_env, algorithm_name)\n",
    "    \n",
    "    # Close evaluation environment\n",
    "    eval_env.close()\n",
    "    \n",
    "    # Plot training results\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    try:\n",
    "        plot_training_results(callback, eval_results, algorithm_name, save_plots=True)\n",
    "        # Move the plot to the correct directory\n",
    "        plot_file = f'{algorithm_name.lower()}_training_progress.png'\n",
    "        if os.path.exists(plot_file):\n",
    "            new_plot_path = os.path.join(save_path, plot_file)\n",
    "            os.makedirs(os.path.dirname(new_plot_path), exist_ok=True)\n",
    "            os.rename(plot_file, new_plot_path)\n",
    "            print(f\"Plot moved to: {new_plot_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Visualization failed: {e}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXPERIMENT SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Algorithm: {algorithm_name}\")\n",
    "    print(f\"Training completed in: {training_time:.2f} seconds\")\n",
    "    \n",
    "    print(f\"Average evaluation reward: {eval_results['avg_reward']:.4f} ± {eval_results['std_reward']:.4f}\")\n",
    "    print(f\"Average packet loss rate: {eval_results['avg_loss_rate']:.4f} ± {eval_results['std_loss_rate']:.4f}\")\n",
    "    print(f\"Average alpha value: {eval_results['avg_alpha']:.4f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return model, eval_results, callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f15035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_experiments():\n",
    "    \"\"\"Run experiments across all environment configurations and algorithms.\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    ALGORITHMS = [\"SAC\", \"PPO\", \"A2C\", \"TD3\"]  # Excluding DQN as requested\n",
    "    ENV_CONFIGS = list(range(3, 8))  # Configs 2-7 only\n",
    "    \n",
    "    # Training parameters\n",
    "    TIMESTEPS = 400000\n",
    "    TIMESTEPS_PER_EPISODE = 1000\n",
    "    N_ENVS = 4\n",
    "    OBVMODE = \"perfect\"\n",
    "\n",
    "    MOVING_AVG_WINDOW = 1000\n",
    "    \n",
    "    # Create results directory with timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_results_dir = f\"Results/AllExperiments_{timestamp}\"\n",
    "    os.makedirs(base_results_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize results tracking\n",
    "    all_results = []\n",
    "    experiment_log = []\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"STARTING COMPREHENSIVE EXPERIMENT SUITE\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"Algorithms: {ALGORITHMS}\")\n",
    "    print(f\"Environment Configs: {ENV_CONFIGS}\")\n",
    "    print(f\"Total Experiments: {len(ALGORITHMS) * len(ENV_CONFIGS)}\")\n",
    "    print(f\"Results Directory: {base_results_dir}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    experiment_count = 0\n",
    "    total_experiments = len(ALGORITHMS) * len(ENV_CONFIGS)\n",
    "    \n",
    "    try:\n",
    "        # All imports and main code here\n",
    "        import torch\n",
    "        for configIdx in ENV_CONFIGS:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"ENVIRONMENT CONFIG {configIdx}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # Get environment configuration\n",
    "            envParams = getEnvConfig(configIdx)\n",
    "            visualizeEnvConfig(envParams)\n",
    "            \n",
    "            # Get predictor configuration\n",
    "            predictorParams = getPredictorConfig(configIdx)\n",
    "            visualizePredictorConfig(predictorParams)\n",
    "            \n",
    "            # Create environment\n",
    "            trafficDataParentPath = f'Results/TrafficData'\n",
    "            try:\n",
    "                simEnv = createEnv(envParams, trafficDataParentPath)\n",
    "                simEnv.selectMode(mode=\"train\", type=\"data\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to create environment for config {configIdx}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            for algorithm in ALGORITHMS:\n",
    "                experiment_count += 1\n",
    "                print(f\"\\n{'-'*60}\")\n",
    "                print(f\"EXPERIMENT {experiment_count}/{total_experiments}: Config {configIdx} + {algorithm}\")\n",
    "                print(f\"{'-'*60}\")\n",
    "                \n",
    "                # Create experiment-specific directories\n",
    "                exp_name = f\"config{configIdx}_{algorithm}\"\n",
    "                save_path = os.path.join(base_results_dir, f\"Config{configIdx}\", algorithm)\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                \n",
    "                # Record experiment start\n",
    "                exp_start_time = datetime.datetime.now()\n",
    "                \n",
    "                try:\n",
    "                    # Run single experiment\n",
    "                    model, eval_results, callback = main_single_experiment(\n",
    "                        envParams,\n",
    "                        simEnv,\n",
    "                        save_path=save_path,\n",
    "                        agent_name=exp_name,\n",
    "                        algorithm_name=algorithm,\n",
    "                        total_timesteps=TIMESTEPS,\n",
    "                        timesteps_per_episode=TIMESTEPS_PER_EPISODE,\n",
    "                        obvMode=OBVMODE,\n",
    "                        n_envs=N_ENVS,\n",
    "                        moving_avg_window=MOVING_AVG_WINDOW\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate experiment duration\n",
    "                    exp_duration = (datetime.datetime.now() - exp_start_time).total_seconds()\n",
    "                    \n",
    "                    # Collect results\n",
    "                    experiment_result = {\n",
    "                        'config_idx': configIdx,\n",
    "                        'algorithm': algorithm,\n",
    "                        'env_config': envParams,\n",
    "                        'avg_reward': eval_results['avg_reward'],\n",
    "                        'std_reward': eval_results['std_reward'],\n",
    "                        'avg_loss_rate': eval_results['avg_loss_rate'],\n",
    "                        'std_loss_rate': eval_results['std_loss_rate'],\n",
    "                        'avg_alpha': eval_results['avg_alpha'],\n",
    "                        'training_timesteps': len(callback.timesteps_log) * callback.log_interval if hasattr(callback, 'timesteps_log') else 0,\n",
    "                        'experiment_duration': exp_duration,\n",
    "                        'save_path': save_path\n",
    "                    }\n",
    "                    \n",
    "                    all_results.append(experiment_result)\n",
    "                    \n",
    "                    # Save individual experiment results\n",
    "                    exp_results_file = os.path.join(save_path, 'experiment_results.json')\n",
    "                    with open(exp_results_file, 'w') as f:\n",
    "                        json.dump(experiment_result, f, indent=2, default=str)\n",
    "                    \n",
    "                    # Save callback data\n",
    "                    if hasattr(callback, 'timesteps_log'):\n",
    "                        callback_data = {\n",
    "                            'timesteps_log': callback.timesteps_log,\n",
    "                            'rewards_log': callback.rewards_log,\n",
    "                            'cumulative_rewards': callback.cumulative_rewards,\n",
    "                        }\n",
    "                        callback_file = os.path.join(save_path, 'training_data.pickle')\n",
    "                        with open(callback_file, 'wb') as f:\n",
    "                            pickle.dump(callback_data, f)\n",
    "                    \n",
    "                    print(f\"✓ Experiment {experiment_count} completed successfully\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Experiment {experiment_count} failed: {e}\")\n",
    "                    # Log the failure\n",
    "                    experiment_result = {\n",
    "                        'config_idx': configIdx,\n",
    "                        'algorithm': algorithm,\n",
    "                        'env_config': envParams,\n",
    "                        'status': 'failed',\n",
    "                        'error': str(e),\n",
    "                        'experiment_duration': (datetime.datetime.now() - exp_start_time).total_seconds()\n",
    "                    }\n",
    "                    all_results.append(experiment_result)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted by user. Exiting gracefully.\")\n",
    "        import sys\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2859ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_all_experiments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic_predictor_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

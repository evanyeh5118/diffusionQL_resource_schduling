{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../../../')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from src.difsched.config import getEnvConfig, visualizeEnvConfig, getDatasetConfig, visualizeDatasetConfig\n",
    "from src.difsched.utils.DataSampler import ReplayBuffer, ReplayBufferHybrid\n",
    "from src.difsched.utils.Visualization import MultiLivePlot\n",
    "from src.difsched.env.EnvironmentSim import createEnv\n",
    "from src.difsched.utils.EnvInterface import EnvInterface\n",
    "from src.difsched.evaluation import eval\n",
    "from src.difsched.agents.DiffusionQL.DQL_Q_esmb import DQL_Q_esmb as Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Environment Configuration\n",
      "==================================================\n",
      "Number of Users:        8\n",
      "Window Length:          200\n",
      "Dataflow:               thumb_fr\n",
      "Sigmoid K List:         [0.1, 0.2, 0.3, 0.4, 0.5]\n",
      "Sigmoid S List:         [10.0, 10.0, 10.0, 10.0, 10.0]\n",
      "Resource Bar:           5\n",
      "Bandwidth:              100\n",
      "Sub Agents:             [[0, 0]]\n",
      "User Map:               [[0, 1, 2, 3], [4, 5, 6, 7]]\n",
      "==================================================\n",
      "==================================================\n",
      "Dataset Configuration\n",
      "==================================================\n",
      "Number of Users:        8\n",
      "Window Length:          200\n",
      "N_aggregation:          4\n",
      "Dataflow:               thumb_fr\n",
      "Random Seed:            999\n",
      "Resource Bar:           5\n",
      "Bandwidth:              100\n",
      "Sigmoid K List:         [0.3]\n",
      "Sigmoid S List:         [10.0]\n",
      "Sub Agents:             [[0, 0]]\n",
      "User Map:               [[0, 1, 2, 3], [4, 5, 6, 7]]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "envConfigIdx = 0\n",
    "envParams = getEnvConfig(envConfigIdx)\n",
    "visualizeEnvConfig(envParams)\n",
    "\n",
    "datasetConfigIdx = 0\n",
    "datasetParams = getDatasetConfig(datasetConfigIdx)\n",
    "visualizeDatasetConfig(datasetParams)\n",
    "\n",
    "trafficDataParentPath = f'../../../../data/raw/traffic'\n",
    "env = createEnv(envParams, trafficDataParentPath)\n",
    "env.selectMode(mode=\"train\", type=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. packet loss rate: 0.33388894431115085\n",
      "length of dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "with open(f'../../../../data/processed/offline_dataset/subOptimalAgent_encConfig{datasetConfigIdx}_{envParams[\"sub_agents_idx\"]}.pkl', 'rb') as f:\n",
    "    dataset_expert = pickle.load(f)\n",
    "\n",
    "dataset_off = {\n",
    "    'observations': dataset_expert['uRecord'],     \n",
    "    'actions': dataset_expert['actionsRecord'], \n",
    "    'rewards': dataset_expert['rewardRecord'], \n",
    "    'next_observations': dataset_expert['uNextRecord']\n",
    "}\n",
    "print(f\"Avg. packet loss rate: {np.mean(dataset_expert['rewardRecord'])}\")\n",
    "print(f\"length of dataset: {len(dataset_off['observations'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'N_diffusion_steps':30,\n",
    "    'schedule_type': \"vp\",\n",
    "    'abs_action_max': 1.0,\n",
    "    'gamma': 0.99,\n",
    "    'lr': 5e-4,\n",
    "    'decay_lr': True,\n",
    "    'weight_decay': 0.0,\n",
    "    'num_critics': 8,\n",
    "    'lcb_coef': 0.15,\n",
    "    'q_sample_eta': 1.0,\n",
    "    'weight_entropy_loss': 0.01,\n",
    "    'weight_q_loss': 1.0,\n",
    "    'approximate_action': True,\n",
    "    'ema_tau': 0.001,\n",
    "    'ema_period': 20,\n",
    "    'ema_begin_update': 1000,\n",
    "    'layer_norm': True,\n",
    "    'grad_clip': 3.0,\n",
    "    'device': 'cuda',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert's Reward: 0.6659311652183533\n",
      "state_dim: 8, action_dim: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================Iteration 10====================\n",
      "Ld: 0.14931574650108814, Lq: 0.6654142016172409, Le: -1.5740736174583434, loss_Q: 0.013272479884326458\n",
      "Avg. Reward: 0.4777179394505329, sample_ratio: 0.95, weight_bc_loss: 1.0\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.43502893197366965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.43322280857138695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.4312438449718464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.41595941435183703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.397285479412988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.3907813070310474\n",
      "====================Iteration 20====================\n",
      "Ld: 0.14865337505936624, Lq: 0.6746866953372955, Le: -1.560660080909729, loss_Q: 0.013502478124573827\n",
      "Avg. Reward: 0.41435471802200025, sample_ratio: 0.9, weight_bc_loss: 1.0\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.3815947336699306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.36830823116879513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.3673121019755942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.36627724732503586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.3618868300853788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.3518281598524299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.350980663229202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================Iteration 30====================\n",
      "Ld: 0.12880088306963444, Lq: 0.683540067076683, Le: -1.5811374354362489, loss_Q: 0.013530754074454308\n",
      "Avg. Reward: 0.3600641174187641, sample_ratio: 0.85, weight_bc_loss: 1.0\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.34981005486966443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.34957105146543044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================Iteration 40====================\n",
      "Ld: 0.1394987626373768, Lq: 0.6890191715955735, Le: -1.5605802893638612, loss_Q: 0.013742734100669623\n",
      "Avg. Reward: 0.3575353112358427, sample_ratio: 0.8, weight_bc_loss: 1.0\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.34598628410732846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.34533213361565407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.33779366731512694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================Iteration 50====================\n",
      "Ld: 0.12360217101871968, Lq: 0.6969855779409408, Le: -1.5810922491550445, loss_Q: 0.01319136838428676\n",
      "Avg. Reward: 0.34802396231324384, sample_ratio: 0.75, weight_bc_loss: 1.0\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert's Reward: 0.6680389046669006\n",
      "state_dim: 8, action_dim: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 45\u001b[0m\n\u001b[0;32m     41\u001b[0m _, explore_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(agent, env, envInterface, LEN_eval\u001b[38;5;241m=\u001b[39mLEN_eval, obvMode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     42\u001b[0m                        sample_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexploration\u001b[39m\u001b[38;5;124m\"\u001b[39m, N_action_candidates\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[0;32m     43\u001b[0m                        eta\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m3.0\u001b[39m), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     44\u001b[0m dataSamplerOn\u001b[38;5;241m.\u001b[39maddOnline(explore_data)\n\u001b[1;32m---> 45\u001b[0m reward, offpolicy_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvInterface\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEN_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEN_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobvMode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredicted\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m                              \u001b[49m\u001b[43msample_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgreedy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_action_candidates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m dataSamplerOn\u001b[38;5;241m.\u001b[39maddOnline(offpolicy_data)\n\u001b[0;32m     48\u001b[0m sample_ratio \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax([min_sp_ratio, max_sp_ratio \u001b[38;5;241m-\u001b[39m ((max_sp_ratio\u001b[38;5;241m-\u001b[39mmin_sp_ratio)\u001b[38;5;241m/\u001b[39mwarm_up_period) \u001b[38;5;241m*\u001b[39m idx_episode])\n",
      "File \u001b[1;32mc:\\Users\\Ye\\Documents\\YuYeh_Documents\\L2S\\Projects\\Diffusion-offRL\\diffusion_resource_schduling_intra_slice\\src\\notebooks\\02_training\\dql_training\\../../../..\\src\\difsched\\evaluation\\Eval.py:67\u001b[0m, in \u001b[0;36meval\u001b[1;34m(agent, env, envInterface, LEN_eval, obvMode, mode, type, sample_method, N_action_candidates, eta, verbose)\u001b[0m\n\u001b[0;32m     65\u001b[0m info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_observations\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m window \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(LEN_eval), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation windows\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m verbose):\n\u001b[1;32m---> 67\u001b[0m     u, action, reward, u_next \u001b[38;5;241m=\u001b[39m \u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvInterface\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobvMode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_action_candidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m#============ Record Results ============\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(u)\n",
      "File \u001b[1;32mc:\\Users\\Ye\\Documents\\YuYeh_Documents\\L2S\\Projects\\Diffusion-offRL\\diffusion_resource_schduling_intra_slice\\src\\notebooks\\02_training\\dql_training\\../../../..\\src\\difsched\\evaluation\\Eval.py:38\u001b[0m, in \u001b[0;36m_step\u001b[1;34m(agent, env, envInterface, obvMode, sample_method, N_action_candidates, eta)\u001b[0m\n\u001b[0;32m     31\u001b[0m     a \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m     32\u001b[0m         s, \n\u001b[0;32m     33\u001b[0m         sample_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEAS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     34\u001b[0m         N\u001b[38;5;241m=\u001b[39mN_action_candidates, \n\u001b[0;32m     35\u001b[0m         eta \u001b[38;5;241m=\u001b[39m eta\n\u001b[0;32m     36\u001b[0m     )\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_action_candidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43meta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     44\u001b[0m r \u001b[38;5;241m=\u001b[39m envInterface\u001b[38;5;241m.\u001b[39mpostprocess_action(a)\n\u001b[0;32m     45\u001b[0m reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mapplyActions(r)\n",
      "File \u001b[1;32mc:\\Users\\Ye\\miniconda3\\envs\\traffic_predictor_3_9\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ye\\Documents\\YuYeh_Documents\\L2S\\Projects\\Diffusion-offRL\\diffusion_resource_schduling_intra_slice\\src\\notebooks\\02_training\\dql_training\\../../../..\\src\\difsched\\agents\\DiffusionQL\\DQL_Q_esmb.py:181\u001b[0m, in \u001b[0;36mDQL_Q_esmb.sample\u001b[1;34m(self, s, sample_method, N, eta)\u001b[0m\n\u001b[0;32m    179\u001b[0m B \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    180\u001b[0m s_rep \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(B, N, s\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, s\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 181\u001b[0m a_cand \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_DDIM\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m q_cand \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;241m.\u001b[39mq_min(s_rep, a_cand)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(B, N)\n\u001b[0;32m    183\u001b[0m a_cand \u001b[38;5;241m=\u001b[39m a_cand\u001b[38;5;241m.\u001b[39mview(B, N, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ye\\Documents\\YuYeh_Documents\\L2S\\Projects\\Diffusion-offRL\\diffusion_resource_schduling_intra_slice\\src\\notebooks\\02_training\\dql_training\\../../../..\\src\\difsched\\agents\\DiffusionQL\\Actors.py:99\u001b[0m, in \u001b[0;36mDiffusionPolicy.sample_DDIM\u001b[1;34m(self, s, eta)\u001b[0m\n\u001b[0;32m     96\u001b[0m alpha_prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschedule\u001b[38;5;241m.\u001b[39malpha_bar[step\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.0\u001b[39m, device\u001b[38;5;241m=\u001b[39ms\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     98\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((B,), step, device\u001b[38;5;241m=\u001b[39ms\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m---> 99\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m sigma_t \u001b[38;5;241m=\u001b[39m eta \u001b[38;5;241m*\u001b[39m ((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha_prev) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha))\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha\u001b[38;5;241m/\u001b[39malpha_prev)\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m#sigma_t = eta *(1 - alpha/alpha_prev).sqrt()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ye\\miniconda3\\envs\\traffic_predictor_3_9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ye\\miniconda3\\envs\\traffic_predictor_3_9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Ye\\Documents\\YuYeh_Documents\\L2S\\Projects\\Diffusion-offRL\\diffusion_resource_schduling_intra_slice\\src\\notebooks\\02_training\\dql_training\\../../../..\\src\\difsched\\agents\\DiffusionQL\\Actors.py:19\u001b[0m, in \u001b[0;36mDiffusionPolicy.forward\u001b[1;34m(self, a_t, s, t)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, a_t: torch\u001b[38;5;241m.\u001b[39mTensor, s: torch\u001b[38;5;241m.\u001b[39mTensor, t: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 19\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[43mtimestep_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps_net(torch\u001b[38;5;241m.\u001b[39mcat([a_t, s, emb], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Ye\\Documents\\YuYeh_Documents\\L2S\\Projects\\Diffusion-offRL\\diffusion_resource_schduling_intra_slice\\src\\notebooks\\02_training\\dql_training\\../../../..\\src\\difsched\\agents\\DiffusionQL\\Helpers.py:29\u001b[0m, in \u001b[0;36mtimestep_embedding\u001b[1;34m(t, dim, max_period)\u001b[0m\n\u001b[0;32m     27\u001b[0m args \u001b[38;5;241m=\u001b[39m t[:, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m*\u001b[39m freq[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     28\u001b[0m emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39mcos(args), torch\u001b[38;5;241m.\u001b[39msin(args)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ye\\miniconda3\\envs\\traffic_predictor_3_9\\lib\\site-packages\\torch\\nn\\functional.py:5096\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(input, pad, mode, value)\u001b[0m\n\u001b[0;32m   5089\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplicate\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   5090\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put.\u001b[39;00m\n\u001b[0;32m   5091\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[0;32m   5092\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[0;32m   5093\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\n\u001b[0;32m   5094\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5095\u001b[0m             )\u001b[38;5;241m.\u001b[39m_replication_pad(\u001b[38;5;28minput\u001b[39m, pad)\n\u001b[1;32m-> 5096\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterations = 100\n",
    "batch_size = 100\n",
    "LEN_eval = 50\n",
    "report_period = 10\n",
    "warm_up_period = 100\n",
    "max_sp_ratio, min_sp_ratio = 1.0, 0.5\n",
    "max_weight_bc_loss, min_weight_bc_loss = 1.0, 1.0\n",
    "rb_capacity = 30000\n",
    "\n",
    "save_folder = f\"../../../../data/results/dql/config_{envConfigIdx}\"\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "for N_exp in range(2):\n",
    "    with open(f\"{save_folder}/hyperparams_{N_exp}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(hyperparams, f)\n",
    "    envInterface = EnvInterface(\n",
    "        envParams, discrete_state=False,\n",
    "        n_bits_state=3, base_state=5,\n",
    "        n_bits_action=2, base_action=200,\n",
    "    )\n",
    "    dataSamplerOff = ReplayBuffer(capacity=rb_capacity, envInterface=envInterface, device=hyperparams['device'])\n",
    "    dataSamplerOn = ReplayBufferHybrid(capacity=rb_capacity, envInterface=envInterface, device=hyperparams['device'])\n",
    "    dataSamplerOff.add(dataset_off)\n",
    "    dataSamplerOn.addOffline(dataset_off)\n",
    "    batch = dataSamplerOff.sample(len(dataSamplerOff))\n",
    "    print(f\"Expert's Reward: {np.mean(batch[2].cpu().detach().numpy())}\")\n",
    "\n",
    "    print(f\"state_dim: {envInterface.state_dim}, action_dim: {envInterface.action_dim}\")\n",
    "    agent = Agent(\n",
    "        state_dim=envInterface.state_dim, \n",
    "        action_dim=envInterface.action_dim, \n",
    "        **hyperparams\n",
    "    )\n",
    "    metrics_train = {'Ld': [], 'Lq': [], 'Le': [], 'loss_Q': [], 'Reward': []}\n",
    "    ploter = MultiLivePlot(nrows=1, ncols=5, titles=[\"Ld\", \"Lq\", \"Le\", \"loss_Q\", \"Reward\"], display_window=25)\n",
    "    best_reward = np.inf\n",
    "    idx_episode = 1\n",
    "    while(True):\n",
    "        metrics = agent.train_split(dataSamplerOff, dataSamplerOn, iterations, batch_size, tqdm_pos=0)\n",
    "        _, explore_data = eval(agent, env, envInterface, LEN_eval=LEN_eval, obvMode=\"predicted\", \n",
    "                               sample_method=\"exploration\", N_action_candidates=10, \n",
    "                               eta=np.random.uniform(0.5, 3.0), verbose=True)\n",
    "        dataSamplerOn.addOnline(explore_data)\n",
    "        reward, offpolicy_data = eval(agent, env, envInterface, LEN_eval=LEN_eval, obvMode=\"predicted\", \n",
    "                                      sample_method=\"greedy\", N_action_candidates=50, eta=1.0, verbose=True)\n",
    "        dataSamplerOn.addOnline(offpolicy_data)\n",
    "        sample_ratio = np.max([min_sp_ratio, max_sp_ratio - ((max_sp_ratio-min_sp_ratio)/warm_up_period) * idx_episode])\n",
    "        dataSamplerOn.set_sample_ratio(sample_ratio)\n",
    "        weight_bc_loss = np.max([min_weight_bc_loss, max_weight_bc_loss - ((max_weight_bc_loss-min_weight_bc_loss)/warm_up_period) * idx_episode])\n",
    "        agent.set_weight_bc_loss(weight_bc_loss)\n",
    "\n",
    "        metrics_train['Ld'] += metrics['Ld']\n",
    "        metrics_train['Lq'] += metrics['Lq']\n",
    "        metrics_train['Le'] += metrics['Le']\n",
    "        metrics_train['loss_Q'] += metrics['loss_Q']\n",
    "        metrics_train['Reward'].append(reward)\n",
    "        ploter.update(0, idx_episode, np.mean(metrics['Ld']))\n",
    "        ploter.update(1, idx_episode, np.mean(metrics['Lq']))\n",
    "        ploter.update(2, idx_episode, np.mean(metrics['Le']))\n",
    "        ploter.update(3, idx_episode, np.mean(metrics['loss_Q']))\n",
    "        ploter.update(4, idx_episode, reward)    \n",
    "      \n",
    "        if idx_episode > 10:\n",
    "            window = 5\n",
    "            smooth_reward = np.convolve(\n",
    "                np.concatenate([np.zeros(window), np.array(metrics_train['Reward'])]), \n",
    "                np.ones(window)/window, mode='valid')[1:]\n",
    "            # save model\n",
    "            if smooth_reward[-1] < best_reward:\n",
    "                best_reward = smooth_reward[-1]\n",
    "                agent.save_model(save_folder, f'{N_exp}_best')\n",
    "                print(f\"save model {N_exp}_best, smoothed reward: {best_reward}\")\n",
    "                with open(f\"{save_folder}/train_metrics_{N_exp}_best.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(metrics_train, f)\n",
    "            # stop training\n",
    "            if np.abs(smooth_reward[-1] - smooth_reward[-2]) < 1e-6 or \\\n",
    "                idx_episode > 50:\n",
    "                #smooth_reward[-1] > 5.0*smooth_reward[-window] or \\\n",
    "                agent.save_model(save_folder, f'{N_exp}_end')\n",
    "                with open(f\"{save_folder}/train_metrics_{N_exp}_end.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(metrics_train, f)\n",
    "                break\n",
    "\n",
    "        if idx_episode % report_period == 0:\n",
    "            print(\"=\" * 20 + f\"Iteration {idx_episode}\" + \"=\" * 20)\n",
    "            print(f\"Ld: {np.mean(metrics['Ld'])}, \" + \n",
    "                f\"Lq: {np.mean(metrics['Lq'])}, \" + \n",
    "                f\"Le: {np.mean(metrics['Le'])}, \" + \n",
    "                f\"loss_Q: {np.mean(metrics['loss_Q'])}\")\n",
    "            print(f\"Avg. Reward: {np.mean(metrics_train['Reward'][-int(report_period):])}, sample_ratio: {sample_ratio}, weight_bc_loss: {weight_bc_loss}\")\n",
    "            print(\"=\" * 50) \n",
    "        idx_episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(5, 2))\n",
    "plt.plot(metrics_train['Reward'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic_predictor_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

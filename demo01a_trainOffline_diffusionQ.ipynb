{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from Configs import getEnvConfig, visualizeEnvConfig, getDatasetConfig, visualizeDatasetConfig\n",
    "from Helpers.DataSampler import ReplayBuffer, ReplayBufferHybrid\n",
    "from Helpers.Visualization import MultiLivePlot\n",
    "from Environment.EnvironmentSim import createEnv\n",
    "from Helpers.EnvInterface import EnvInterface\n",
    "from Helpers.Eval import eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Environment Configuration\n",
      "==================================================\n",
      "Number of Users:        20\n",
      "Window Length:          200\n",
      "Dataflow:               thumb_bk\n",
      "Sigmoid K List:         [0.1, 0.2, 0.3, 0.4, 0.5]\n",
      "Sigmoid S List:         [10.0, 10.0, 10.0, 10.0, 10.0]\n",
      "Resource Bar:           5\n",
      "Bandwidth:              200\n",
      "Sub Agents:             [[1, 1, 1, 1, 1]]\n",
      "User Map:               [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19]]\n",
      "==================================================\n",
      "==================================================\n",
      "Dataset Configuration\n",
      "==================================================\n",
      "Number of Users:        20\n",
      "Window Length:          200\n",
      "N_aggregation:          4\n",
      "Dataflow:               thumb_bk\n",
      "Random Seed:            999\n",
      "Resource Bar:           5\n",
      "Bandwidth:              200\n",
      "Sigmoid K List:         [0.3]\n",
      "Sigmoid S List:         [10.0]\n",
      "Sub Agents:             [[1, 1, 1, 1, 1]]\n",
      "User Map:               [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19]]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "envConfigIdx = 0\n",
    "envParams = getEnvConfig(envConfigIdx)\n",
    "visualizeEnvConfig(envParams)\n",
    "\n",
    "datasetConfigIdx = 0\n",
    "datasetParams = getDatasetConfig(datasetConfigIdx)\n",
    "visualizeDatasetConfig(datasetParams)\n",
    "\n",
    "trafficDataParentPath = f'Datasets/TrafficDataset/TrafficData'\n",
    "env = createEnv(envParams, trafficDataParentPath)\n",
    "env.selectMode(mode=\"train\", type=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. packet loss rate: 0.4738505039223643\n",
      "length of dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "with open(f'Datasets/OfflineDataset/subOptimalAgent_encConfig{datasetConfigIdx}_{envParams[\"sub_agents_idx\"]}.pkl', 'rb') as f:\n",
    "    dataset_expert = pickle.load(f)\n",
    "with open(f'Datasets/OfflineDataset/random_policy_8_users.pkl', 'rb') as f:\n",
    "    dataset_random = pickle.load(f)\n",
    "\n",
    "dataset_off = {\n",
    "    'observations': dataset_expert['uRecord'],     \n",
    "    'actions': dataset_expert['actionsRecord'], \n",
    "    'rewards': dataset_expert['rewardRecord'], \n",
    "    'next_observations': dataset_expert['uNextRecord']\n",
    "}\n",
    "print(f\"Avg. packet loss rate: {np.mean(dataset_expert['rewardRecord'])}\")\n",
    "print(f\"length of dataset: {len(dataset_off['observations'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'N_diffusion_steps':30,\n",
    "    'schedule_type': \"vp\",\n",
    "    'abs_action_max': 1.0,\n",
    "    'gamma': 0.99,\n",
    "    'lr': 5e-4,\n",
    "    'decay_lr': True,\n",
    "    'weight_decay': 0.0,\n",
    "    'num_critics': 8,\n",
    "    'lcb_coef': 0.15,\n",
    "    'q_sample_eta': 1.0,\n",
    "    'weight_entropy_loss': 0.01,\n",
    "    'weight_q_loss': 1.0,\n",
    "    'approximate_action': True,\n",
    "    'ema_tau': 0.001,\n",
    "    'ema_period': 20,\n",
    "    'ema_begin_update': 1000,\n",
    "    'layer_norm': True,\n",
    "    'grad_clip': 3.0,\n",
    "    'device': 'cuda',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert's Reward: 0.5262174606323242\n",
      "state_dim: 20, action_dim: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================Iteration 10====================\n",
      "Ld: 0.12396391712129116, Lq: 0.526014432311058, Le: -1.5293437361717224, loss_Q: 0.006541097303852439\n",
      "Avg. Reward: 0.6593772399266051, sample_ratio: 0.95, weight_bc_loss: 1.0\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.628411368070088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.6192674180088923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.6055249510095051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.5973702406721723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.5831596307912654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.5658016138638471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.5512020907208797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.5436932253077297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.5270384094274878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.5209711251268689\n",
      "====================Iteration 20====================\n",
      "Ld: 0.11964490413665771, Lq: 0.5363591969013214, Le: -1.528318407535553, loss_Q: 0.007012844723649323\n",
      "Avg. Reward: 0.5520653779590672, sample_ratio: 0.9, weight_bc_loss: 1.0\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.5164472935627933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.5090382101036461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.5034561687229804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.5013228182084882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.4970780470789471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.4908715358813738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.49065478378567756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.4870471809219912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.4844641398705061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.48191409421057146\n",
      "====================Iteration 30====================\n",
      "Ld: 0.09851015970110893, Lq: 0.5432795852422714, Le: -1.5392620730400086, loss_Q: 0.006659077354706824\n",
      "Avg. Reward: 0.4894960706447593, sample_ratio: 0.85, weight_bc_loss: 1.0\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================Iteration 40====================\n",
      "Ld: 0.10239594727754593, Lq: 0.5480386954545975, Le: -1.5258098673820495, loss_Q: 0.006758152116090059\n",
      "Avg. Reward: 0.4890375066857301, sample_ratio: 0.8, weight_bc_loss: 1.0\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model 0_best, smoothed reward: 0.47790722317044615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================Iteration 50====================\n",
      "Ld: 0.0861209562420845, Lq: 0.5477991890907288, Le: -1.5268995261192322, loss_Q: 0.006729420279152691\n",
      "Avg. Reward: 0.4826101521465467, sample_ratio: 0.75, weight_bc_loss: 1.0\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert's Reward: 0.5266821384429932\n",
      "state_dim: 20, action_dim: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m idx_episode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 41\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataSamplerOff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataSamplerOn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtqdm_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     _, explore_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(agent, env, envInterface, LEN_eval\u001b[38;5;241m=\u001b[39mLEN_eval, obvMode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     43\u001b[0m                            sample_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexploration\u001b[39m\u001b[38;5;124m\"\u001b[39m, N_action_candidates\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[0;32m     44\u001b[0m                            eta\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m3.0\u001b[39m), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     45\u001b[0m     dataSamplerOn\u001b[38;5;241m.\u001b[39maddOnline(explore_data)\n",
      "File \u001b[1;32mc:\\Users\\Ye\\Documents\\YuYeh_Documents\\L2S\\Projects\\diffusion_resource_schduling_intra_slice\\DiffusionQL\\DQL_Q_esmb.py:165\u001b[0m, in \u001b[0;36mDQL_Q_esmb.train_split\u001b[1;34m(self, offline_buffer, online_buffer, iterations, batch_size, tqdm_pos)\u001b[0m\n\u001b[0;32m    163\u001b[0m batch_offline \u001b[38;5;241m=\u001b[39m offline_buffer\u001b[38;5;241m.\u001b[39msample(batch_size)\n\u001b[0;32m    164\u001b[0m batch_online \u001b[38;5;241m=\u001b[39m online_buffer\u001b[38;5;241m.\u001b[39msample(batch_size)\n\u001b[1;32m--> 165\u001b[0m Ld, Lq, Le, loss_Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_offline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_online\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLd\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(Ld)\n\u001b[0;32m    167\u001b[0m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLq\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(Lq)\n",
      "File \u001b[1;32mc:\\Users\\Ye\\Documents\\YuYeh_Documents\\L2S\\Projects\\diffusion_resource_schduling_intra_slice\\DiffusionQL\\DQL_Q_esmb.py:135\u001b[0m, in \u001b[0;36mDQL_Q_esmb.update\u001b[1;34m(self, batch_off, batch_on)\u001b[0m\n\u001b[0;32m    133\u001b[0m loss_pi \u001b[38;5;241m=\u001b[39m L_bc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_q_loss \u001b[38;5;241m*\u001b[39m L_q \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_entropy_loss \u001b[38;5;241m*\u001b[39m L_e\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_actor\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 135\u001b[0m \u001b[43mloss_pi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_clip, norm_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_actor\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Ye\\miniconda3\\envs\\traffic_predictor_3_9\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ye\\miniconda3\\envs\\traffic_predictor_3_9\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ye\\miniconda3\\envs\\traffic_predictor_3_9\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterations = 100\n",
    "batch_size = 100\n",
    "LEN_eval = 50\n",
    "report_period = 10\n",
    "warm_up_period = 100\n",
    "max_sp_ratio, min_sp_ratio = 1.0, 0.5\n",
    "max_weight_bc_loss, min_weight_bc_loss = 1.0, 1.0\n",
    "rb_capacity = 30000\n",
    "\n",
    "save_folder = f\"Models/config_{envConfigIdx}\"\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "for N_exp in range(2):\n",
    "    with open(f\"Models/config_{envConfigIdx}/hyperparams_{N_exp}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(hyperparams, f)\n",
    "    envInterface = EnvInterface(\n",
    "        envParams, discrete_state=False,\n",
    "        n_bits_state=3, base_state=5,\n",
    "        n_bits_action=2, base_action=200,\n",
    "    )\n",
    "    dataSamplerOff = ReplayBuffer(capacity=rb_capacity, envInterface=envInterface, device=hyperparams['device'])\n",
    "    dataSamplerOn = ReplayBufferHybrid(capacity=rb_capacity, envInterface=envInterface, device=hyperparams['device'])\n",
    "    dataSamplerOff.add(dataset_off)\n",
    "    dataSamplerOn.addOffline(dataset_off)\n",
    "    batch = dataSamplerOff.sample(len(dataSamplerOff))\n",
    "    print(f\"Expert's Reward: {np.mean(batch[2].cpu().detach().numpy())}\")\n",
    "\n",
    "    from DiffusionQL.DQL_Q_esmb import DQL_Q_esmb as Agent\n",
    "    print(f\"state_dim: {envInterface.state_dim}, action_dim: {envInterface.action_dim}\")\n",
    "    agent = Agent(\n",
    "        state_dim=envInterface.state_dim, \n",
    "        action_dim=envInterface.action_dim, \n",
    "        **hyperparams\n",
    "    )\n",
    "    metrics_train = {'Ld': [], 'Lq': [], 'Le': [], 'loss_Q': [], 'Reward': []}\n",
    "    ploter = MultiLivePlot(nrows=1, ncols=5, titles=[\"Ld\", \"Lq\", \"Le\", \"loss_Q\", \"Reward\"], display_window=25)\n",
    "    best_reward = np.inf\n",
    "    idx_episode = 1\n",
    "    while(True):\n",
    "        metrics = agent.train_split(dataSamplerOff, dataSamplerOn, iterations, batch_size, tqdm_pos=0)\n",
    "        _, explore_data = eval(agent, env, envInterface, LEN_eval=LEN_eval, obvMode=\"predicted\", \n",
    "                               sample_method=\"exploration\", N_action_candidates=10, \n",
    "                               eta=np.random.uniform(0.5, 3.0), verbose=True)\n",
    "        dataSamplerOn.addOnline(explore_data)\n",
    "        reward, offpolicy_data = eval(agent, env, envInterface, LEN_eval=LEN_eval, obvMode=\"predicted\", \n",
    "                                      sample_method=\"greedy\", N_action_candidates=50, eta=1.0, verbose=True)\n",
    "        dataSamplerOn.addOnline(offpolicy_data)\n",
    "        sample_ratio = np.max([min_sp_ratio, max_sp_ratio - ((max_sp_ratio-min_sp_ratio)/warm_up_period) * idx_episode])\n",
    "        dataSamplerOn.set_sample_ratio(sample_ratio)\n",
    "        weight_bc_loss = np.max([min_weight_bc_loss, max_weight_bc_loss - ((max_weight_bc_loss-min_weight_bc_loss)/warm_up_period) * idx_episode])\n",
    "        agent.set_weight_bc_loss(weight_bc_loss)\n",
    "\n",
    "        metrics_train['Ld'] += metrics['Ld']\n",
    "        metrics_train['Lq'] += metrics['Lq']\n",
    "        metrics_train['Le'] += metrics['Le']\n",
    "        metrics_train['loss_Q'] += metrics['loss_Q']\n",
    "        metrics_train['Reward'].append(reward)\n",
    "        ploter.update(0, idx_episode, np.mean(metrics['Ld']))\n",
    "        ploter.update(1, idx_episode, np.mean(metrics['Lq']))\n",
    "        ploter.update(2, idx_episode, np.mean(metrics['Le']))\n",
    "        ploter.update(3, idx_episode, np.mean(metrics['loss_Q']))\n",
    "        ploter.update(4, idx_episode, reward)    \n",
    "      \n",
    "        if idx_episode > 10:\n",
    "            window = 5\n",
    "            smooth_reward = np.convolve(\n",
    "                np.concatenate([np.zeros(window), np.array(metrics_train['Reward'])]), \n",
    "                np.ones(window)/window, mode='valid')[1:]\n",
    "            # save model\n",
    "            if smooth_reward[-1] < best_reward:\n",
    "                best_reward = smooth_reward[-1]\n",
    "                agent.save_model(save_folder, f'{N_exp}_best')\n",
    "                print(f\"save model {N_exp}_best, smoothed reward: {best_reward}\")\n",
    "                with open(save_folder + f\"/train_metrics_{N_exp}_best.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(metrics_train, f)\n",
    "            # stop training\n",
    "            if np.abs(smooth_reward[-1] - smooth_reward[-2]) < 1e-6 or \\\n",
    "                idx_episode > 50:\n",
    "                #smooth_reward[-1] > 5.0*smooth_reward[-window] or \\\n",
    "                agent.save_model(save_folder, f'{N_exp}_end')\n",
    "                with open(save_folder + f\"/train_metrics_{N_exp}_end.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(metrics_train, f)\n",
    "                break\n",
    "\n",
    "        if idx_episode % report_period == 0:\n",
    "            print(\"=\" * 20 + f\"Iteration {idx_episode}\" + \"=\" * 20)\n",
    "            print(f\"Ld: {np.mean(metrics['Ld'])}, \" + \n",
    "                f\"Lq: {np.mean(metrics['Lq'])}, \" + \n",
    "                f\"Le: {np.mean(metrics['Le'])}, \" + \n",
    "                f\"loss_Q: {np.mean(metrics['loss_Q'])}\")\n",
    "            print(f\"Avg. Reward: {np.mean(metrics_train['Reward'][-int(report_period):])}, sample_ratio: {sample_ratio}, weight_bc_loss: {weight_bc_loss}\")\n",
    "            print(\"=\" * 50) \n",
    "        idx_episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(5, 2))\n",
    "plt.plot(metrics_train['Reward'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic_predictor_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
